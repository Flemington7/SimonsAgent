\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Enhanced Quantitative Trading Strategies through Sentiment Analysis Using Large Language Models}

\author{\IEEEauthorblockN{1\textsuperscript{st} Wentao Ye}
\IEEEauthorblockA{\textit{SIGS} \\
\textit{Tsinghua University}\\
Shenzhen, China \\
yewt23@mails.tsinghua.edu.cn}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Huaxuan Li}
\IEEEauthorblockA{\textit{SIGS} \\
\textit{Tsinghua University}\\
Shenzhen, China \\
lhx23@mails.tsinghua.edu.cn}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Jiadong Li}
\IEEEauthorblockA{\textit{SIGS} \\
\textit{Tsinghua University}\\
Shenzhen, China \\
lijd23@mails.tsinghua.edu.cn}
}

\maketitle

\begin{abstract}
The stock market's significant price volatility necessitates sophisticated and adaptive trading strategies. 
Traditional quantitative trading strategies relying on historical market data face limitations. 
This project integrates sentiment analysis using large language models (LLMs) to enhance quantitative trading strategies. 
By incorporating real-time sentiment analysis from news articles and social media, we aim to improve trading performance, decision-making, and overall profitability in quantitative trading.
\end{abstract}

\begin{IEEEkeywords}
Quantitative Finance, Sentiment Analysis, Large Language Models, Stock Market, Trading Strategies
\end{IEEEkeywords}

\section{\textbf{Introduction}}

\subsection{\textbf{Strategy Premise}}

\subsubsection{\textbf{Characteristics of the Stock Market}}
The stock market is characterized by its significant price volatility driven by factors such as market sentiment, speculative trading, and news events. This volatility creates opportunities for profit but also introduces risk, necessitating sophisticated trading strategies.

\subsubsection{\textbf{Limitations of Traditional Quantitative Trading Strategies}}
Traditional strategies rely on historical data and often neglect emotional and psychological factors. These strategies struggle to incorporate unstructured data from social media and news, presenting an opportunity for enhancement through sentiment analysis.

\section{\textbf{Method}}
\subsection{\textbf{Overview of the Workflow}}

Our project integrates sentiment analysis with traditional price-volume trading strategies to enhance Quantitative Trading Strategies.
The workflow consists of three main components: Information Extraction, Decision Generation, and Execution. 
The design of the workflow is illustrated in Fig. \ref{fig:workflow}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{workflow.jpg}
    \caption{Quantitative Trading Strategies with Sentiment Analysis Workflow.}
    \label{fig:workflow}
\end{figure}

\subsubsection{Information Extraction}

This stage involves gathering and processing both textual and numerical data to generate actionable factors for trading models.

\paragraph{Textual Data Processing:}
\begin{itemize}
    \item \textbf{Data Source:} Textual data is sourced from news articles and social media platforms such as Twitter.
    \item \textbf{Fine-Tuning:} The collected textual data is processed using a fine-tuned large language model, Llama 3, enhanced with Low-Rank Adaptation (LoRA) to efficiently adapt to sentiment analysis tasks.
    \item \textbf{Sentiment Scoring:} The fine-tuned Llama 3 model generates sentiment scores from the textual data, reflecting market sentiment towards various stocks or market conditions.
    \item \textbf{Sentiment Factor:} These sentiment scores are then transformed into sentiment factors, which are used as input features for the trading model.
\end{itemize}

\paragraph{Numerical Data Processing:}
\begin{itemize}
    \item \textbf{Data Source:} Numerical data includes price and volume information for various stocks.
    \item \textbf{Price-Volume Features:} Key features such as Exponential Moving Average (EMA), Volume Weighted Average Price (VWAP), and Min-Max Range are derived from the numerical data. These factors capture essential market dynamics and trends.
\end{itemize}

\paragraph{Combined Features Creation:}
\begin{itemize}
    \item \textbf{Integration:} Sentiment factors and price-volume features are combined to form a comprehensive set of input features for the trading model. This integrated approach allows the model to consider both market sentiment and quantitative metrics.
\end{itemize}

\subsubsection{Decision Generation}

In this stage, the derived features are used to generate trading signals and manage the portfolio.

\paragraph{Forecast Model:}
\begin{itemize}
    \item \textbf{Model Selection:} We utilize the LightGBM model, a gradient boosting framework, to forecast future price movements and generate trading signals based on the combined features.
    \item \textbf{Stock Ranking List:} The LightGBM model ranks the stocks, indicating which stocks are likely to perform well based on the input features.
\end{itemize}

\paragraph{Portfolio Management:}
\begin{itemize}
    \item \textbf{Strategy Implementation:} \texttt{TopkDropoutStrategy} is used to manage the portfolio. This strategy involves buying and selling a fixed number of stocks daily based on their ranked trading signals.
\end{itemize}

\subsubsection{Execution}

The final component involves executing the trading decisions generated in the previous stage.

\subsection{\textbf{Information Extraction}}

\subsubsection{\textbf{LLM Fine-Tuning Methods Based on Sentiment Data}}
In our project, we leverage LoRA to fine-tune Llama 3, a state-of-the-art large language model.
Fine-tuning improves the LLM model's performance in specific tasks, such as sentiment analysis, by adapting it to domain-specific data.
LoRA, a parameter-efficient fine-tuning (PEFT) method, which is designed to reduce the computational cost of fine-tuning large models.
Traditional fine-tuning of models like Llama 3 can be computationally expensive and resource-intensive. 
LoRA addresses this challenge by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks, making the fine-tuning process faster and less resource-demanding.
Despite the reduced computational load, LoRA maintains high performance in task-specific adjustments \cite{hu2021}.
Specifically, the fine-tuning process starts by initializing the pre-trained Llama 3 model. 
The pre-trained weights provide a solid foundation, having been trained on diverse and extensive corpus data. 
LoRA layers are then integrated into Llama 3. 
These layers are low-rank matrices that capture task-specific adjustments, minimizing computational costs by introducing low-rank matrices \( A \in \mathbb{R}^{d \times r} \) and \( B \in \mathbb{R}^{r \times k} \) to approximate the original weight matrix \( W \in \mathbb{R}^{d \times k} \), where \( r \) is the rank of the decomposition, and \( r \ll \min(d, k) \).
Instead of updating all model parameters, LoRA adds the product of these matrices to the original weight matrix \( W \), as \( W' = W + A \times B \). 
During the fine-tuning process, only the matrices \(A\) and \(B\) are trained, which significantly reduces the number of parameters to be updated, making the process more efficient, as shown in Fig. \ref{fig:lora}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{lora.png}
    \caption{The Low-rank Adaptation Method.}
    \label{fig:lora}
\end{figure}

\subsubsection{\textbf{Factor Generated by Sentiment Analysis}}
Once fine-tuned, Llama 3 is integrated into our sentiment analysis pipeline. 
The model processes textual data from news or social media, classifying sentiments accurately and efficiently. 
The estimated sentiment value can therefore be used for generate sentiment factors, where we use explonantial weighted moving average to account for the time decay of the sentiment information.

\subsubsection{\textbf{Features Combination with Traditional Quantitative Features}}
The original price-volume data consists of opening price, closing price, highest price, lowest price, trading volume, and restoration factor. 
With this information, various features can be created.
The features encompass various aspects of market data, technical indicators, and statistical measures, enabling a holistic understanding of market dynamics. 
A total of 158 features have been created using 'data loader' provided by Qlib.

Here we treat the sentiment factor as a new type of feature, and combine it with traditional quantitative price-volume features to create a hybrid feature set.

\subsection{\textbf{Decision Generation}}

In the prediction model section, the current choice is the LightGBM model, which is a gradient boosting framework based on decision trees. 
During the training process, LightGBM constructs a series of decision trees iteratively, where each tree is built to minimize the residual errors of the previous trees. 
The model ranks features based on their importance scores, which indicate the contribution of each feature to the prediction. 
These importance scores can be used to generate a stock ranking list, displaying the significance of each stock in the model \cite{ke2017}.

After generating the ranking list, we can use a simple rule-based \texttt{TopkDropoutStrategy} to determine the target amount of each stock. 
The strategy adopts the Topk-Drop algorithm, which sells and buys a fixed number of stocks every trading day based on the prediction scores. 
The detailed steps of the \texttt{TopkDropoutStrategy} are as follows:

\begin{itemize}
    \item Adopt the Topk-Drop algorithm to calculate the target amount of each stock.
    
    There are two parameters for the Topk-Drop algorithm:
    \begin{itemize}
        \item \textbf{Topk}: The number of stocks held.
        \item \textbf{Drop}: The number of stocks sold on each trading day.
    \end{itemize}
    
    In general, the number of stocks currently held is \texttt{Topk}, with the exception of being zero at the beginning period of trading. 
    For each trading day, let $d$ be the number of the instruments currently held and with a rank $K$ when ranked by the prediction scores from high to low. 
    Then $d$ number of stocks currently held with the worst prediction score will be sold, and the same number of unheld stocks with the best prediction score will be bought.
    
    In general, $d=$ \texttt{Drop}, especially when the pool of the candidate instruments is large, $K$ is large, and \texttt{Drop} is small.
    
    In most cases, the Topk-Drop algorithm sells and buys \texttt{Drop} stocks every trading day, which yields a turnover rate of $2 \times \texttt{Drop} / K$.
    
    The following images illustrate a typical scenario.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.4\textwidth]{topk_drop.png}
        \caption{TopkDropoutStrategy}
        \label{fig:TopkDropoutStrategy}
    \end{figure}

    \item Generate the order list from the target amount, and the order list is then sent to the Execution Environment for execution.
\end{itemize}

\section{\textbf{Backtesting Results}}

\begin{itemize}
    \item \textbf{Time Period:} January 1, 2020, to August 1, 2022.
    \item \textbf{Benchmark:} SPY, the SPDR S\&P 500 ETF trust, designed to track the S\&P 500 stock market index.
    \item \textbf{Transaction Cost:} Commission of 0.0005.
\end{itemize}

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\textwidth]{plot_0.png}
\caption{Comprehensive Report}
\label{fig:comprehensive report}
\end{figure}

\subsection*{\textbf{Cumulative Returns (Cum Bench, Cum Return Wo Cost, Cum Return W Cost)}}

\textbf{Cum Bench:}
This line represents the cumulative returns of the benchmark, which is the SPY (SPDR S\&P 500 ETF Trust) in this case. The SPY tracks the S\&P 500 index, providing a benchmark for comparing the portfolio's performance.

\textbf{Cum Return Wo Cost:}
This line shows the cumulative returns of the portfolio without accounting for transaction costs.

\textbf{Cum Return W Cost:}
This line represents the cumulative returns of the portfolio after accounting for transaction costs (commission of 0.0005).

\subsection*{\textbf{Maximum Drawdown (Return Wo MDD, Return W Cost MDD)}}
\textbf{Return Wo MDD:}
Maximum drawdown series of the portfolio's cumulative return without transaction costs. Maximum drawdown measures the largest peak-to-trough decline in the portfolio's value.

\textbf{Return W Cost MDD:}
Similar to "Return Wo MDD" but includes the impact of transaction costs.

\subsection*{\textbf{Cumulative Abnormal Return (Cum Ex Return Wo Cost, Cum Ex Return W Cost)}}

\textbf{Cum Ex Return Wo Cost:}
The CAR (cumulative abnormal return) series of the portfolio compared to the benchmark, without considering transaction costs. It shows the excess return generated by the portfolio relative to the benchmark.

\textbf{Cum Ex Return W Cost:}
Similar to "Cum Ex Return Wo Cost" but includes transaction costs.

\subsection*{Turnover}
\textbf{Turnover:}
This line represents the portfolio's turnover rate, which is the frequency of buying and selling securities within the portfolio.

\subsection*{\textbf{Drawdown Series of CAR (Cum Ex Return W Cost MDD, Cum Ex Return Wo Cost MDD)}}
\textbf{Cum Ex Return W Cost MDD:}
Drawdown series of the CAR with transaction costs included.

\textbf{Cum Ex Return Wo Cost MDD:}
Similar to the above but without transaction costs.

\begin{center}
\subsection*{\textbf{Backtesting Performance Overview}}
\end{center}

Our quantitative model's backtesting performance from January 1, 2020, to August 1, 2022, as shown in the Fig.~\ref{fig:comprehensive report}, indicates different performance characteristics. Here are possible reasons for performance variations:

\begin{enumerate}
    \item \textbf{Market Conditions}
    \begin{itemize}
        \item \textbf{January to April 2020:}
        \begin{itemize}
            \item \textbf{COVID-19 Pandemic:} The outbreak of COVID-19 led to extreme market volatility and a sharp decline in equity markets worldwide. Traditional models and strategies often struggle during periods of unprecedented events and high volatility. Even if our sentiment factor fine-tuned model still perform badly. This could explain the underperformance relative to SPY during this period.
            \item \textbf{Market Crash:} The market experienced a sudden crash in March 2020. Many quantitative models, especially those not specifically designed for extreme events, may have suffered during this time.
        \end{itemize}
        \item \textbf{May 2020 onwards:}
        \begin{itemize}
            \item \textbf{COVID-19 Pandemic:} From May 2020, markets began to recover as governments and central banks around the world implemented stimulus measures. This period saw a strong rebound in equity markets.
            \item \textbf{Market Crash:} Our model might be more effective during market recovery periods. If it is based on momentum, mean reversion, or other strategies that capitalize on trending markets, it would benefit from the sustained recovery and upward trend post-April 2020.
        \end{itemize}
    \end{itemize}

    \item \textbf{Strategy-Specific Factors}
    \begin{itemize}
        \item \textbf{January to April 2020:}
        \begin{itemize}
            \item \textbf{Strategy Limitations:} Our strategy relies on stable market conditions or specific economic indicators, it might have struggled during the early 2020 period of uncertainty and rapid changes.
            \item \textbf{Lagging Indicators:} Some strategies use indicators that lag behind actual market movements. During rapid declines, these models might not adjust quickly enough.
        \end{itemize}
        \item \textbf{May 2020 onwards:}
        \begin{itemize}
            \item \textbf{Effective Signal Processing:} Post-April 2020, as the market conditions stabilized and trends became more predictable, our model have been able to effectively process signals and generate excess returns.
            \item \textbf{Rebalancing and Optimization:} Since our model that include periodic rebalancing or dynamic optimization, it may cause perform better once the initial shock has passed and market signals are clearer.
        \end{itemize}
    \end{itemize}

    \item \textbf{Risk Management and Cost}
    \begin{itemize}
        \item \textbf{January to April 2020:}
        \begin{itemize}
            \item \textbf{High Volatility:} Increased volatility can lead to higher transaction costs and slippage, negatively impacting performance.
            \item \textbf{Risk Management Constraints:} Stringent risk management protocols might have resulted in more conservative positioning, leading to underperformance relative to a benchmark that does not adjust for risk as actively.
        \end{itemize}
        \item \textbf{May 2020 onwards:}
        \begin{itemize}
            \item \textbf{Improved Conditions:} As volatility decreased and markets trended upwards, the impact of transaction costs and slippage would be less severe.
            \item \textbf{Optimized Risk Management:} The model’s risk management strategies might have been more effective during stable market conditions, allowing for better performance relative to the benchmark.
        \end{itemize}
    \end{itemize}

\end{enumerate}

\begin{center}
    \subsection*{\textbf{Annualized Return}}
\end{center}

The annualized return measures the geometric average amount of money earned by an investment each year over a given time period, as shown in Fig.~\ref{fig:annualized return}.

\begin{figure}[h!]
\centering
    \includegraphics[width=0.5\textwidth]{annualized_return.png}
    \caption{Annualized Return}
    \label{fig:annualized return}
\end{figure}

\textbf{Analysis:}
\begin{itemize}
    \item \textbf{Volatility and Peaks:} The high volatility and peaks early in the chart (around April 2020) suggest that the model was capturing significant short-term gains. This might be attributed to the high market volatility due to the COVID-19 pandemic, where models that are responsive to rapid market movements can capture substantial returns.
    \item \textbf{Stabilization:} Post-2020, the annualized return stabilizes. This indicates that the model adapts well to varying market conditions, capturing consistent excess returns. The stability suggests effective signal processing and risk management mechanisms within the model.
    \item \textbf{Cost Impact:} The close alignment of returns with and without costs indicates low transaction costs, which is beneficial for high-frequency trading models.
\end{itemize}

\begin{center}
    \subsection*{\textbf{Maximum Drawdown}}
\end{center}

Maximum drawdown represents the largest drop from a peak to a trough before a new peak is attained. It is a critical measure of downside risk, as shown in Fig.~\ref{fig:maximum drawdown}.

\begin{figure}[h!]
\centering
    \includegraphics[width=0.5\textwidth]{max_drawdown.png}
    \caption{Maximum Drawdown}
    \label{fig:maximum drawdown}
\end{figure}

\textbf{Analysis:}
\begin{itemize}
    \item \textbf{Initial Drawdown:} The sharp drawdown in early 2020 corresponds with the market crash due to the pandemic. This significant drawdown highlights the model's initial struggle during extreme market conditions.
    \item \textbf{Risk Management:} The reduced drawdowns post-2020 indicate improved risk management. The model's ability to minimize losses after the initial market shock suggests effective adaptation to market recovery.
    \item \textbf{Drawdown Consistency:} The similar patterns of drawdowns with and without costs show that the cost factor has a minimal impact on risk during this period, which is a positive sign for the strategy's robustness and prove that our workflow is cost-efficient in high-frequency trading.
\end{itemize}

\begin{center}
    \subsection*{\textbf{Information Ratio}}
\end{center}

The information ratio is a measure of portfolio returns beyond the returns of a benchmark, usually an index, here we use SP500 as the benchmark, as shown in Fig.~\ref{fig:information ratio}.

\begin{figure}[h!]
\centering
    \includegraphics[width=0.5\textwidth]{information_ratio.png}
    \caption{Information Ratio}
    \label{fig:information ratio}
\end{figure}

\textbf{Analysis:}
\begin{itemize}
    \item \textbf{Performance Peaks:} Peaks in the information ratio indicate periods where the model generated high risk-adjusted returns, notably around April 2020 and early 2021. This shows the model's effectiveness in generating excess returns relative to the risk taken during these periods.
    \item \textbf{Consistent Performance:} The consistent positive information ratio over time reflects the model's ability to continuously generate alpha. This is crucial for strategies aimed at outperforming benchmarks consistently. At the most time, the information ratio is above 0, which means our model is outperforming the benchmark.
    \item \textbf{Cost Impact:} The close tracking of the information ratio with and without costs suggests that the strategy is cost-efficient, maintaining high risk-adjusted returns even after accounting for transaction costs.
\end{itemize}

\begin{center}
    \subsection*{\textbf{Standard Deviation}}
\end{center}

Standard deviation measures the amount of variability or dispersion around the mean return of an investment, as shown in Fig.~\ref{fig:standard deviation}.

\begin{figure}[h!]
\centering
    \includegraphics[width=0.5\textwidth]{std.png}
    \caption{Standard Deviation}
    \label{fig:standard deviation}
\end{figure}

\textbf{Analysis:}
\begin{itemize}
    \item \textbf{High Initial Volatility:} Early 2020 shows high standard deviation, indicating significant volatility due to the market turmoil caused by the pandemic.
    \item \textbf{Decreasing Volatility:} The reduction in standard deviation over time signifies a stabilization of the model's returns. Lower volatility post-2020 suggests that the market conditions became more predictable and the model adapted to these conditions.
    \item \textbf{Model Stability:} The declining standard deviation implies that the model's risk (in terms of return variability) has decreased, which is a positive sign of its robustness and reliability over time.
\end{itemize}

\section{\textbf{Conclusion}}

\subsection{\textbf{Problem Analysis}}

The backtesting performance of our quantitative model from January 1, 2020, to August 1, 2022, highlighted several key areas of concern and areas for improvement:

\begin{itemize}
    \item \textbf{Initial Underperformance}: From January to April 2020, the model underperformed relative to the benchmark (SPY). This period coincided with the onset of the COVID-19 pandemic, which caused extreme market volatility and sharp declines. The model struggled to adapt to these unprecedented conditions, resulting in significant drawdowns and underperformance.
    \item \textbf{Volatility Sensitivity}: The model exhibited high sensitivity to market volatility. During periods of high volatility, such as the market crash in March 2020, the model's performance deteriorated, leading to significant drawdowns. This indicates that the model's risk management and volatility adjustment mechanisms were insufficient during extreme market conditions.
    \item \textbf{Single Model Limitation}: The current strategy relies solely on a decision-tree-based model, specifically LightGBM. While LightGBM is powerful, it may not be the optimal choice for all market conditions, especially when dealing with a diverse set of factors such as price-volume combinations and sentiment factors.
\end{itemize}

\subsection{\textbf{Proposed Improvements}}

To address the identified issues and enhance the model's performance, several improvements are proposed:

\begin{itemize}
    \item \textbf{Diversify Prediction Models}: Relying on a single model (LightGBM) limits the strategy's adaptability and robustness. To improve performance, we should experiment with and incorporate multiple models, including but not limited to:
    \begin{itemize}
        \item \textbf{Random Forests}: For their robustness and ability to handle noisy data.
        \item \textbf{Gradient Boosting Machines}: Besides LightGBM, explore alternatives like XGBoost and CatBoost.
        \item \textbf{Neural Networks}: Especially deep learning models that can capture complex patterns and interactions in the data.
        \item \textbf{Support Vector Machines (SVM)}: For their effectiveness in high-dimensional spaces.
    \end{itemize}
    \item \textbf{Enhanced Volatility Management}: Implement advanced volatility-adjusted strategies to better manage risk during periods of high market volatility. Techniques such as dynamic volatility scaling, regime-switching models, or incorporating volatility indices (e.g., VIX) can help the model adapt to changing market conditions.
    \item \textbf{Sentiment Analysis Integration}: While our current model incorporates sentiment factors, there is room for improvement in how these factors are integrated. Advanced natural language processing (NLP) techniques, such as transformers (e.g., BERT, GPT-3), can be utilized to extract more nuanced sentiment signals from textual data, potentially improving the model's predictive power.
    \item \textbf{Regular Model Evaluation and Tuning}: Establish a robust framework for continuous model evaluation and hyperparameter tuning. This includes implementing automated machine learning (AutoML) techniques to regularly assess and optimize model parameters, ensuring the strategy remains aligned with current market dynamics.
    \item \textbf{Feature Engineering and Selection}: Enhance the feature engineering process to identify and incorporate additional predictive factors. Techniques such as feature importance analysis, principal component analysis (PCA), and clustering can help identify and select the most relevant features, improving model accuracy and robustness.
\end{itemize}

\subsection{\textbf{Additional Recommendations}}

\begin{itemize}
    \item \textbf{Incorporate Twitter Sentiment Analysis}: Beyond analyzing news sentiment, incorporating sentiment analysis of social media platforms like Twitter could provide valuable insights. Since individual investors' sentiments on platforms like Twitter may often be reactionary and less informed, they could serve as contrarian indicators. Conversely, authoritative news sources can be used as positive indicators due to their more reliable and analyzed content.
    \item \textbf{Utilize Specialized Sentiment Models}: While large models like Llama 3 have strong generalization capabilities, they might not always outperform specialized models in niche domains. Using smaller, domain-specific sentiment analysis models fine-tuned for financial texts may yield better results in extracting relevant sentiment signals from financial news and social media content.
\end{itemize}

\section{\textbf{Contribution}}

\begin{thebibliography}{00}
    \bibitem{yang2020} X. Yang, W. Liu, D. Zhou, J. Bian, and T.-Y. Liu, “Qlib: An AI-oriented Quantitative Investment Platform.” arXiv, Sep. 22, 2020. Accessed: Jun. 21, 2024. [Online]. Available: http://arxiv.org/abs/2009.11189
    \bibitem{hu2021} E. J. Hu et al., “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv, Oct. 16, 2021. Accessed: Jun. 27, 2024. [Online]. Available: http://arxiv.org/abs/2106.09685
    \bibitem{ke2017} G. Ke et al., “LightGBM: A Highly Efficient Gradient Boosting Decision Tree”.
\end{thebibliography}

\end{document}
